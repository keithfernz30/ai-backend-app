# ğŸš€ AI Backend API

An async FastAPI-based AI backend with task routing, mock mode support, latency tracking, and deployment-ready configuration.

---

## ğŸ“Œ Overview

This project demonstrates a production-style AI backend architecture featuring:

* âš¡ Async FastAPI backend
* ğŸ§  Task-based request routing
* ğŸ§ª Mock mode (no API cost during development)
* ğŸ¤– Real OpenAI integration
* ğŸ“¦ JSON response enforcement
* â±ï¸ Latency measurement
* ğŸ“Š Token usage tracking
* ğŸŒ Simple frontend interface
* â˜ï¸ Cloud deployment ready

---

## ğŸ—ï¸ Architecture

```
Frontend (HTML + JavaScript)
        â†“
FastAPI Backend (Async)
        â†“
Mock NLP Logic   OR   OpenAI API
```

The backend supports two operating modes:

* ğŸ§ª **Mock Mode** â€“ Local testing without API usage
* ğŸ¤– **Real Mode** â€“ Calls OpenAI API for actual responses

---

## ğŸ”Œ API Endpoint

### POST `/analyze`

Request body:

```json
{
  "text": "Keith is very happy",
  "task": "keywords"
}
```

Supported tasks:

* ğŸ“ summarize
* ğŸ˜Š sentiment
* ğŸ”‘ keywords

---

## ğŸ“¤ Example Response

```json
{
  "mode": "mock",
  "task": "keywords",
  "result": {
    "keywords": ["Keith", "very happy"],
    "count": 2
  },
  "latency_ms": 9.31,
  "tokens": {
    "prompt_tokens": 4,
    "completion_tokens": 20,
    "total_tokens": 24
  }
}
```

---

## âœ¨ Features

### ğŸ§­ Task Routing

A single endpoint dynamically routes logic based on task type.

### ğŸ§  Mock NLP Engine

Includes:

* Proper noun detection
* Phrase extraction (e.g., "very happy")
* Stopword filtering
* Rule-based sentiment scoring

### â±ï¸ Latency Tracking

Each request returns processing time in milliseconds.

### ğŸ“Š Token Usage Tracking

* Simulated token counts in mock mode
* Real usage data in OpenAI mode

---

## ğŸ› ï¸ Tech Stack

* ğŸ Python 3.9+
* âš¡ FastAPI
* ğŸš€ Uvicorn
* ğŸ¤– OpenAI SDK (Async)
* ğŸŒ HTML + Vanilla JavaScript
* â˜ï¸ Render (Deployment)

---

## ğŸ§‘â€ğŸ’» Local Setup

Clone the repository:

```bash
git clone https://github.com/keithfernz30/ai-backend-app.git
cd ai-backend-app
```

Create virtual environment:

```bash
python -m venv venv
source venv/bin/activate
```

Install dependencies:

```bash
pip install -r requirements.txt
```

Create `.env` file:

```
OPENAI_API_KEY=your_api_key_here
```

Run the server:

```bash
python -m uvicorn main:app --reload
```

Open in browser:

```
http://127.0.0.1:8000
```

---

## ğŸ”„ Switching Between Mock & Real Mode

Inside `main.py`:

```python
USE_MOCK = True
```

Set:

* ğŸ§ª `True` â†’ Mock mode
* ğŸ¤– `False` â†’ Real OpenAI mode

---

## â˜ï¸ Deployment (Render)

Build Command:

```
pip install -r requirements.txt
```

Start Command:

```
uvicorn main:app --host 0.0.0.0 --port 10000
```

Add environment variable in Render dashboard:

```
OPENAI_API_KEY=your_key
```

---

## ğŸ“‚ Project Structure

```
ai-backend-app/
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ static/
    â””â”€â”€ index.html
```

---

## ğŸ¯ What This Project Demonstrates

* âš¡ Async backend design
* ğŸ§  Feature flag architecture (mock vs real)
* ğŸ“¦ Structured JSON enforcement
* ğŸ¤– AI API integration
* ğŸ“Š Latency & token monitoring
* ğŸš€ Deployment workflow

---

## ğŸ‘¨â€ğŸ’» Author

Keith Fernandes

---
